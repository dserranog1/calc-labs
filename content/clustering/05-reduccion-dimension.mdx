---
title: Reducci칩n de dimensi칩n
sidebarTitle: Reducci칩n de dimensi칩n
---

<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex/dist/katex.css"
/>



## 游댷 쯇or qu칠 necesitamos reducir la dimensi칩n?

Hasta ahora, hemos aprendido que:

- Cada imagen de un d칤gito escrito a mano se representa como un **vector de 64 n칰meros**.
- Esos vectores viven en un espacio de **64 dimensiones**.
- Cada imagen es, entonces, un **punto en $\mathbb{R}^{64}$**.

Y nuestro objetivo es claro:

> **Agrupar las im치genes que representan d칤gitos similares, sin saber de antemano qu칠 n칰mero es cada una.**

---

### 游 Pero... 쯖칩mo vemos esos grupos?

Queremos saber si las im치genes del n칰mero "3" est치n cerca unas de otras, si los "7" se separan bien de los "1", etc.  
Pero hay un problema:

> **No podemos visualizar ni razonar directamente en 64 dimensiones.**

No podemos "ver" c칩mo est치n distribuidos esos vectores en ese espacio tan grande.

---

### 游꿢 La soluci칩n: proyectar los datos a 2 dimensiones

Necesitamos transformar los datos a un espacio que **s칤 podamos visualizar**: un plano.  
Queremos una funci칩n que lleve:

```math
\mathbb{R}^{64} \longrightarrow \mathbb{R}^2
```

Pero no cualquier transformaci칩n. Queremos una que **conserve la estructura**:  
- Que mantenga separados los grupos que realmente son distintos.
- Que coloque cerca los puntos que se parecen.

---

### 游 쯈u칠 herramienta usaremos?

Utilizaremos **PCA** (An치lisis de Componentes Principales), una t칠cnica matem치tica que:

- Encuentra las **direcciones m치s importantes** del espacio original.
- Proyecta los datos en esas direcciones, ordenadas seg칰n cu치nta variaci칩n explican.
- Permite tomar solo las **primeras 2 componentes principales**, y graficar los puntos all칤.

As칤, convertimos nuestros vectores de 64 dimensiones en puntos de 2 dimensiones,  
y podremos **visualizar los posibles grupos o cl칰steres** de d칤gitos.

---

> Esta visualizaci칩n no solo nos ayuda a entender el comportamiento de los datos,  
> sino que tambi칠n nos permitir치 aplicar algoritmos de agrupamiento (como **K-Means**) para que la m치quina **descubra esos grupos autom치ticamente**.


### 游빑 Un ejemplo num칠rico simple de PCA

Supongamos que tenemos los siguientes datos, con solo dos caracter칤sticas: ancho y largo de objetos.

```python
import numpy as np
import matplotlib.pyplot as plt

# Datos ficticios: cada fila es [ancho, largo]
X = np.array([
    [1.0, 1.1],
    [2.0, 2.0],
    [3.0, 2.9],
    [4.0, 4.2],
    [5.0, 5.1],
])

# Graficar los puntos
plt.scatter(X[:, 0], X[:, 1], c='blue')
plt.xlabel("Ancho")
plt.ylabel("Largo")
plt.title("Datos originales")
plt.axis("equal")
plt.grid(True)
plt.show()
```

---

### 游 쯈u칠 observa PCA en estos datos?

Aunque los datos est치n en 2D, **est치n casi alineados en diagonal**.  
PCA nota que la mayor variaci칩n no est치 sobre el eje X ni sobre el eje Y, sino **en la direcci칩n diagonal** (como si fuera una nueva regla inclinada).

---

### 游꿢 쯈u칠 hace PCA?

- Encuentra ese eje diagonal (la "componente principal").
- Proyecta los puntos sobre 칠l.
- Puede incluso **descartar la segunda dimensi칩n**, si aporta muy poca variaci칩n.

---

### 游늴 Resultado: datos en 1D

```python
from sklearn.decomposition import PCA

# Aplicar PCA para reducir a 1 dimensi칩n
pca = PCA(n_components=1)
X_reduced = pca.fit_transform(X)

# Mostrar los datos proyectados
print("Datos proyectados sobre la componente principal:")
print(X_reduced)
```

> Aunque originalmente ten칤amos dos variables (ancho y largo), ahora cada objeto se representa con **un solo n칰mero**, su posici칩n sobre la componente principal.

---

Este es exactamente el tipo de transformaci칩n que aplicaremos a los vectores de las im치genes:  
reducir sus 64 dimensiones a solo 2, **conservando la estructura de los datos** para poder visualizarlos.


### 游꿢 Visualizando la proyecci칩n sobre la componente principal

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Datos simples: ancho y largo
X = np.array([
    [1.0, 1.1],
    [2.0, 2.0],
    [3.0, 2.9],
    [4.0, 4.2],
    [5.0, 5.1],
])

# Ajustar PCA
pca = PCA(n_components=1)
X_reduced = pca.fit_transform(X)
X_projected = pca.inverse_transform(X_reduced)

# Graficar los puntos originales
plt.figure(figsize=(6,6))
plt.scatter(X[:, 0], X[:, 1], c='blue', label='Puntos originales')

# Graficar las proyecciones
plt.scatter(X_projected[:, 0], X_projected[:, 1], c='red', label='Proyecciones', marker='x')

# Dibujar l칤neas de proyecci칩n
for original, projected in zip(X, X_projected):
    plt.plot([original[0], projected[0]], [original[1], projected[1]], 'k--', linewidth=0.5)

# Dibujar la direcci칩n de la componente principal
origin = np.mean(X, axis=0)
direction = pca.components_[0]
scale = 3  # para dibujar la flecha m치s visible
plt.quiver(*origin, *direction, scale=1/scale, scale_units='xy', angles='xy', color='green', width=0.01, label='Componente principal')

plt.title("Proyecci칩n de los puntos sobre la componente principal")
plt.axis("equal")
plt.grid(True)
plt.legend()
plt.show()
```

> La **l칤nea verde** indica la direcci칩n de mayor variaci칩n encontrada por PCA.  
> Los puntos rojos son las **proyecciones** de los puntos originales (azules) sobre ese eje.
> Como puedes ver, PCA "aplana" los datos en esa direcci칩n para resumirlos.
