---
title: Reducci√≥n de dimensi√≥n
sidebarTitle: Reducci√≥n de dimensi√≥n
---

<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex/dist/katex.css"
/>



## üîΩ ¬øPor qu√© necesitamos reducir la dimensi√≥n?

Hasta ahora, hemos aprendido que:

- Cada imagen de un d√≠gito escrito a mano se representa como un **vector de 64 n√∫meros**.
- Esos vectores viven en un espacio de **64 dimensiones**.
- Cada imagen es, entonces, un **punto en $\mathbb{R}^{64}$**.

Y nuestro objetivo es claro:

> **Agrupar las im√°genes que representan d√≠gitos similares, sin saber de antemano qu√© n√∫mero es cada una.**

---

### üëÄ Pero... ¬øc√≥mo vemos esos grupos?

Queremos saber si las im√°genes del n√∫mero "3" est√°n cerca unas de otras, si los "7" se separan bien de los "1", etc.  
Pero hay un problema:

> **No podemos visualizar ni razonar directamente en 64 dimensiones.**

No podemos "ver" c√≥mo est√°n distribuidos esos vectores en ese espacio tan grande.

---

### üéØ La soluci√≥n: proyectar los datos a 2 dimensiones

Necesitamos transformar los datos a un espacio que **s√≠ podamos visualizar**: un plano.  
Queremos una funci√≥n que lleve:

```math
\mathbb{R}^{64} \longrightarrow \mathbb{R}^2
```

Pero no cualquier transformaci√≥n. Queremos una que **conserve la estructura**:  
- Que mantenga separados los grupos que realmente son distintos.
- Que coloque cerca los puntos que se parecen.

---

### üß† ¬øQu√© herramienta usaremos?

Utilizaremos **PCA** (An√°lisis de Componentes Principales), una t√©cnica matem√°tica que:

- Encuentra las **direcciones m√°s importantes** del espacio original.
- Proyecta los datos en esas direcciones, ordenadas seg√∫n cu√°nta variaci√≥n explican.
- Permite tomar solo las **primeras 2 componentes principales**, y graficar los puntos all√≠.

As√≠, convertimos nuestros vectores de 64 dimensiones en puntos de 2 dimensiones,  
y podremos **visualizar los posibles grupos o cl√∫steres** de d√≠gitos.

---

> Esta visualizaci√≥n no solo nos ayuda a entender el comportamiento de los datos,  
> sino que tambi√©n nos permitir√° aplicar algoritmos de agrupamiento (como **K-Means**) para que la m√°quina **descubra esos grupos autom√°ticamente**.


### üßÆ Un ejemplo num√©rico simple de PCA

Supongamos que tenemos los siguientes datos, con solo dos caracter√≠sticas: ancho y largo de objetos.

```python
import numpy as np
import matplotlib.pyplot as plt

# Datos ficticios: cada fila es [ancho, largo]
X = np.array([
    [1.0, 1.1],
    [2.0, 2.0],
    [3.0, 2.9],
    [4.0, 4.2],
    [5.0, 5.1],
])

# Graficar los puntos
plt.scatter(X[:, 0], X[:, 1], c='blue')
plt.xlabel("Ancho")
plt.ylabel("Largo")
plt.title("Datos originales")
plt.axis("equal")
plt.grid(True)
plt.show()
```

---

### üß† ¬øQu√© observa PCA en estos datos?

Aunque los datos est√°n en 2D, **est√°n casi alineados en diagonal**.  
PCA nota que la mayor variaci√≥n no est√° sobre el eje X ni sobre el eje Y, sino **en la direcci√≥n diagonal** (como si fuera una nueva regla inclinada).

---

### üéØ ¬øQu√© hace PCA?

- Encuentra ese eje diagonal (la "componente principal").
- Proyecta los puntos sobre √©l.
- Puede incluso **descartar la segunda dimensi√≥n**, si aporta muy poca variaci√≥n.

---

### üìâ Resultado: datos en 1D

```python
from sklearn.decomposition import PCA

# Aplicar PCA para reducir a 1 dimensi√≥n
pca = PCA(n_components=1)
X_reduced = pca.fit_transform(X)

# Mostrar los datos proyectados
print("Datos proyectados sobre la componente principal:")
print(X_reduced)
```

> Aunque originalmente ten√≠amos dos variables (ancho y largo), ahora cada objeto se representa con **un solo n√∫mero**, su posici√≥n sobre la componente principal.

---

Este es exactamente el tipo de transformaci√≥n que aplicaremos a los vectores de las im√°genes:  
reducir sus 64 dimensiones a solo 2, **conservando la estructura de los datos** para poder visualizarlos.
