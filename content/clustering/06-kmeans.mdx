---
title: K-Means y centroides
sidebarTitle: K-Means y centroides
---

<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex/dist/katex.css"
/>

## ğŸ§© Aplicando todo lo aprendido: agrupando dÃ­gitos reales

Hasta ahora hemos entendido:

- CÃ³mo representar imÃ¡genes como vectores.
- QuÃ© significa que esos vectores vivan en un espacio de alta dimensiÃ³n.
- Por quÃ© necesitamos proyectarlos a 2 dimensiones para poder analizarlos.
- Y cÃ³mo PCA nos permite hacer esa proyecciÃ³n conservando estructura.

Pero aÃºn no hemos respondido la pregunta clave de este laboratorio:

> **Â¿Puede una mÃ¡quina aprender a agrupar los dÃ­gitos escritos a mano sin saber quÃ© nÃºmero es cada uno?**

Esa es precisamente la tarea del algoritmo **K-Means**, una tÃ©cnica de **aprendizaje no supervisado** que intentaremos aplicar sobre los dÃ­gitos reales.

---

### ğŸ¯ Nuestro objetivo final

Vamos a:

1. Cargar un conjunto de imÃ¡genes reales de dÃ­gitos (de 0 a 9).
2. Representarlas como vectores.
3. Aplicar **PCA** para reducirlas a 2 dimensiones.
4. Aplicar **K-Means** para que la mÃ¡quina **descubra los grupos** por sÃ­ sola.
5. Visualizar los grupos y los **centroides** calculados por el algoritmo.

En el proceso, veremos cÃ³mo el concepto de **centroide**, que hemos estudiado en cÃ¡lculo, se convierte en una herramienta clave para que una mÃ¡quina pueda aprender a organizar datos complejos.

Ahora sÃ­, pongamos todo en acciÃ³n.

---

### ğŸ”¹ Paso 1 â€” Cargar el dataset de dÃ­gitos

Primero, vamos a cargar el conjunto de datos `digits`, que viene incluido en la librerÃ­a `scikit-learn`. Este dataset contiene 1797 imÃ¡genes de dÃ­gitos escritos a mano, cada una de **8Ã—8 pÃ­xeles**, representando nÃºmeros del 0 al 9.

```python
from sklearn.datasets import load_digits

# Cargar el dataset
digits = load_digits()

# Inspeccionar su contenido
print("Cantidad de imÃ¡genes:", len(digits.images))
print("DimensiÃ³n de cada imagen:", digits.images[0].shape)
print("Etiqueta del primer dÃ­gito:", digits.target[0])
```

> Cada imagen es una matriz de $8 \times 8$, y cada etiqueta es un nÃºmero del 0 al 9 que indica quÃ© dÃ­gito representa esa imagen.

---

### ğŸ”¹ Paso 2 â€” Visualizar algunas imÃ¡genes

Vamos a mostrar algunas imÃ¡genes reales para que puedas ver cÃ³mo se ven los datos con los que vamos a trabajar.

```python
import matplotlib.pyplot as plt

# Mostrar las primeras 10 imÃ¡genes con sus etiquetas
fig, axes = plt.subplots(1, 10, figsize=(10, 3))
for i, ax in enumerate(axes):
    ax.imshow(digits.images[i], cmap="gray_r")  # fondo blanco, trazo negro
    ax.set_title(digits.target[i])
    ax.axis("off")
plt.suptitle("Ejemplos del dataset Digits")
plt.show()
```

> Como puedes ver, cada nÃºmero estÃ¡ escrito de forma distinta.  
> Lo interesante serÃ¡ ver si la mÃ¡quina es capaz de descubrir que hay gruposâ€¦ sin saber aÃºn quÃ© nÃºmero representa cada imagen.


### ğŸ”¹ Paso 3 â€” Convertir las imÃ¡genes a vectores y aplicar PCA

Sabemos que cada imagen es una matriz de 8 Ã— 8 pÃ­xeles.  
Para aplicar algoritmos como PCA y K-Means, necesitamos representar cada imagen como un **vector de 64 componentes** (una fila con todos los pÃ­xeles).

> A este proceso se le llama *flattening*, y es simplemente reorganizar los valores de la matriz en una sola fila.

```python
from sklearn.decomposition import PCA

# Aplanar las imÃ¡genes (de matriz 8x8 a vector de 64)
X = digits.data  # matriz de tamaÃ±o (1797, 64)

print("TamaÃ±o del conjunto de datos:", X.shape)
```

---

Ahora que tenemos los datos como vectores en un espacio de 64 dimensiones, aplicamos **PCA** para reducirlos a solo 2 dimensiones y poder graficarlos.

```python
# Aplicar PCA para reducir de 64D a 2D
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

print("TamaÃ±o de los datos proyectados:", X_pca.shape)
```

> Cada imagen ahora es un punto en $\mathbb{R}^2$ (un plano), lo que nos permitirÃ¡ **ver los datos y buscar grupos visualmente**.


### ğŸ”¹ Paso 4 â€” Visualizar los dÃ­gitos proyectados en 2D

Ya que cada imagen ha sido reducida a un punto en 2 dimensiones, podemos graficar todos los datos y observar si existen **agrupamientos naturales**.

Por ahora, usaremos las **etiquetas verdaderas** solo para colorear los puntos y facilitar la observaciÃ³n.  
Recuerda: mÃ¡s adelante usaremos K-Means **sin estas etiquetas**.

```python
# Visualizar los puntos proyectados con colores segÃºn su etiqueta real
plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=digits.target, cmap="tab10", s=15)
plt.xlabel("Componente principal 1")
plt.ylabel("Componente principal 2")
plt.title("DÃ­gitos proyectados a 2D con PCA")
plt.colorbar(scatter, label="Etiqueta real (solo para visualizaciÃ³n)")
plt.grid(True)
plt.show()
```

> Como puedes ver, ciertos grupos ya empiezan a distinguirse:  
> algunos dÃ­gitos como los "0", "1" o "6" tienden a estar **mÃ¡s juntos**,  
> mientras que otros se mezclan un poco mÃ¡s.

Este grÃ¡fico nos da una idea visual de que **sÃ­ existen estructuras o clÃºsteres** en los datos, aunque aÃºn no hemos usado ningÃºn algoritmo para descubrirlos.

> Y ahora que los podemos verâ€¦ **es momento de dejar que la mÃ¡quina intente agruparlos por sÃ­ sola**.


### ğŸ¤– K-Means: un algoritmo para descubrir grupos

Ya tenemos los datos de nuestras imÃ¡genes de dÃ­gitos representados como puntos en un plano.  
Ahora queremos que la mÃ¡quina **descubra por sÃ­ sola** los grupos o clÃºsteres de dÃ­gitos que estÃ¡n mÃ¡s cerca entre sÃ­.

Para esto, usaremos un algoritmo llamado **K-Means**.

---

### ğŸ§  Â¿QuÃ© hace K-Means?

K-Means intenta **dividir los datos en $k$ grupos (clÃºsteres)**, de forma que:

- Los puntos **dentro de un mismo grupo estÃ©n lo mÃ¡s cerca posible entre sÃ­**, y
- Los puntos de **grupos distintos estÃ©n lo mÃ¡s separados posible**.

Lo hace repitiendo dos pasos simples:

1. **AsignaciÃ³n**: cada punto se asigna al **centroide mÃ¡s cercano**.
2. **ActualizaciÃ³n**: cada centroide se recalcula como el **promedio de todos los puntos asignados** a su grupo.

Este proceso se repite hasta que los grupos ya no cambian.

---

### ğŸ“ Â¿QuÃ© es un centroide?

El **centroide** es el punto que representa â€œel centroâ€ de un grupo.  
MatemÃ¡ticamente, si un grupo contiene los puntos $x_1, x_2, ..., x_n$, su centroide es:

```math
\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i
```

> Es el mismo concepto de punto medio o punto de equilibrio que estudiamos en cÃ¡lculo, pero aplicado aquÃ­ al conjunto de puntos en un clÃºster.

---

> Lo interesante es que K-Means **no conoce las etiquetas reales** (los nÃºmeros) â€” solo ve los vectores en el plano y trata de organizarlos.

Ahora vamos a aplicarlo con $k = 10$, ya que sabemos que nuestro conjunto contiene 10 clases distintas (del 0 al 9),  
y observaremos si la mÃ¡quina logra descubrir esos grupos usando Ãºnicamente distancias y promedios.


### ğŸ”¹ Paso 5 â€” Aplicar K-Means y visualizar los clÃºsteres

Usaremos la implementaciÃ³n de `KMeans` de `scikit-learn`, indicando que queremos dividir los datos en **10 grupos**.

```python
from sklearn.cluster import KMeans

# Aplicar K-Means con 10 clÃºsteres
kmeans = KMeans(n_clusters=10, random_state=42)
kmeans.fit(X_pca)

# Etiquetas asignadas por el algoritmo
labels = kmeans.labels_

# Coordenadas de los centroides descubiertos
centroids = kmeans.cluster_centers_
```

---

Ahora graficamos los puntos con colores segÃºn el clÃºster al que fueron asignados,  
y dibujamos los centroides en el plano para ver dÃ³nde estÃ¡n ubicados.

```python
plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap="tab10", s=15)
plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, marker='X', label='Centroides')
plt.xlabel("Componente principal 1")
plt.ylabel("Componente principal 2")
plt.title("ClÃºsteres encontrados por K-Means")
plt.legend()
plt.grid(True)
plt.show()
```

> Los puntos de colores representan los grupos descubiertos.  
> Las **X negras** son los **centroides**, calculados como el promedio de cada grupo de puntos.

---

A pesar de no haber visto ninguna etiqueta real, el algoritmo ha agrupado muchos dÃ­gitos similares juntos.  
Esto muestra cÃ³mo, usando solo la nociÃ³n de **distancia** y el concepto de **centroide**, una mÃ¡quina puede **organizar y entender datos complejos**.



### ğŸ” Paso 6 â€” Comparar los clÃºsteres con las etiquetas reales

Aunque K-Means **no usÃ³ las etiquetas reales**, podemos comparar los clÃºsteres que encontrÃ³ con las verdaderas clases (los dÃ­gitos del 0 al 9) para ver **quÃ© tan bien agrupÃ³** los datos.

Usaremos una **matriz de contingencia** para comparar las asignaciones de K-Means con las etiquetas originales.

```python
from sklearn.metrics import confusion_matrix
import seaborn as sns
import pandas as pd

# Crear matriz de confusiÃ³n entre etiquetas reales y clÃºsteres asignados
conf_mat = confusion_matrix(digits.target, labels)

# Mostrar como heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_mat, annot=True, fmt="d", cmap="Blues", xticklabels=range(10), yticklabels=range(10))
plt.xlabel("ClÃºster asignado por K-Means")
plt.ylabel("Etiqueta real")
plt.title("Matriz de comparaciÃ³n: etiquetas reales vs clÃºsteres")
plt.show()
```

---

### ğŸ“Š MÃ©trica de rendimiento mÃ¡s intuitiva: exactitud por mayorÃ­a de voto

Para cada clÃºster hallado por K-Means, podemos ver quÃ© dÃ­gito real aparece mÃ¡s frecuentemente dentro de ese clÃºster.  
Luego, asumimos que el clÃºster representa ese dÃ­gito, y calculamos **quÃ© porcentaje del total fue correctamente asignado**.

```python
from scipy.stats import mode
import numpy as np

# Crear un arreglo vacÃ­o para las predicciones corregidas
new_labels = np.zeros_like(labels)

# Para cada clÃºster, asignar la etiqueta real mÃ¡s comÃºn (voto mayoritario)
for i in range(10):
    mask = labels == i
    if np.any(mask):  # evitar clÃºsteres vacÃ­os
        new_labels[mask] = mode(digits.target[mask], keepdims=False).mode

# Calcular el accuracy final
accuracy = np.mean(new_labels == digits.target)
print(f"Exactitud (accuracy) del agrupamiento: {accuracy:.2%}")
```

> Esta mÃ©trica responde directamente a la pregunta:  
> **Â¿QuÃ© porcentaje de las imÃ¡genes fueron correctamente agrupadas, si interpretamos cada clÃºster como un nÃºmero?**

---

### ğŸ§  ConclusiÃ³n

A pesar de no haber usado etiquetas reales, K-Means logrÃ³ identificar grupos que **en muchos casos corresponden bien con los dÃ­gitos reales**.

- Algunos clÃºsteres coinciden fuertemente con una sola clase (por ejemplo, los "0" o los "1").
- Otros se mezclan mÃ¡s, especialmente si dos dÃ­gitos se escriben de forma parecida.

Esto muestra tanto el poder como la limitaciÃ³n del aprendizaje no supervisado:

> âœ… K-Means puede descubrir patrones sin saber quÃ© representan.  
> âš ï¸ Pero no siempre entiende el significado semÃ¡ntico de los datos, solo su geometrÃ­a.

En este laboratorio hemos visto cÃ³mo **una idea sencilla como el centroide**, que ya dominamos en cÃ¡lculo, se convierte en una herramienta central para que una mÃ¡quina pueda **entender y organizar datos reales**.


## ğŸ§  Preguntas para reflexionar

Antes de cerrar el laboratorio, responde brevemente las siguientes preguntas. No se espera una respuesta perfecta: lo importante es que puedas expresar lo que comprendiste y lo que te llamÃ³ la atenciÃ³n.

---

1. Â¿Por quÃ© representamos las imÃ¡genes como vectores antes de usarlas en el algoritmo?

2. Â¿Para quÃ© usamos PCA en este laboratorio? Â¿QuÃ© problema resuelve?

3. Â¿QuÃ© papel cumple el **centroide** en el algoritmo K-Means? Â¿En quÃ© se parece al que usamos en cÃ¡lculo?

4. Â¿K-Means usÃ³ las etiquetas reales para agrupar los datos? Â¿CÃ³mo logrÃ³ formar grupos sin saber quÃ© nÃºmero era cada imagen?

5. El algoritmo agrupÃ³ correctamente cerca del 57â€¯% de los datos. Â¿Te parece un buen resultado? Â¿Por quÃ©?

6. Â¿En quÃ© otros contextos del mundo real podrÃ­as aplicar un algoritmo como K-Means?

