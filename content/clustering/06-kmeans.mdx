---
title: K-Means y centroides
sidebarTitle: K-Means y centroides
---

<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex/dist/katex.css"
/>

## üß© Aplicando todo lo aprendido: agrupando d√≠gitos reales

Hasta ahora hemos entendido:

- C√≥mo representar im√°genes como vectores.
- Qu√© significa que esos vectores vivan en un espacio de alta dimensi√≥n.
- Por qu√© necesitamos proyectarlos a 2 dimensiones para poder analizarlos.
- Y c√≥mo PCA nos permite hacer esa proyecci√≥n conservando estructura.

Pero a√∫n no hemos respondido la pregunta clave de este laboratorio:

> **¬øPuede una m√°quina aprender a agrupar los d√≠gitos escritos a mano sin saber qu√© n√∫mero es cada uno?**

Esa es precisamente la tarea del algoritmo **K-Means**, una t√©cnica de **aprendizaje no supervisado** que intentaremos aplicar sobre los d√≠gitos reales.

---

### üéØ Nuestro objetivo final

Vamos a:

1. Cargar un conjunto de im√°genes reales de d√≠gitos (de 0 a 9).
2. Representarlas como vectores.
3. Aplicar **PCA** para reducirlas a 2 dimensiones.
4. Aplicar **K-Means** para que la m√°quina **descubra los grupos** por s√≠ sola.
5. Visualizar los grupos y los **centroides** calculados por el algoritmo.

En el proceso, veremos c√≥mo el concepto de **centroide**, que hemos estudiado en c√°lculo, se convierte en una herramienta clave para que una m√°quina pueda aprender a organizar datos complejos.

Ahora s√≠, pongamos todo en acci√≥n.

---

### üîπ Paso 1 ‚Äî Cargar el dataset de d√≠gitos

Primero, vamos a cargar el conjunto de datos `digits`, que viene incluido en la librer√≠a `scikit-learn`. Este dataset contiene 1797 im√°genes de d√≠gitos escritos a mano, cada una de **8√ó8 p√≠xeles**, representando n√∫meros del 0 al 9.

```python
from sklearn.datasets import load_digits

# Cargar el dataset
digits = load_digits()

# Inspeccionar su contenido
print("Cantidad de im√°genes:", len(digits.images))
print("Dimensi√≥n de cada imagen:", digits.images[0].shape)
print("Etiqueta del primer d√≠gito:", digits.target[0])
```

> Cada imagen es una matriz de $8 \times 8$, y cada etiqueta es un n√∫mero del 0 al 9 que indica qu√© d√≠gito representa esa imagen.

---

### üîπ Paso 2 ‚Äî Visualizar algunas im√°genes

Vamos a mostrar algunas im√°genes reales para que puedas ver c√≥mo se ven los datos con los que vamos a trabajar.

```python
import matplotlib.pyplot as plt

# Mostrar las primeras 10 im√°genes con sus etiquetas
fig, axes = plt.subplots(1, 10, figsize=(10, 3))
for i, ax in enumerate(axes):
    ax.imshow(digits.images[i], cmap="gray_r")  # fondo blanco, trazo negro
    ax.set_title(digits.target[i])
    ax.axis("off")
plt.suptitle("Ejemplos del dataset Digits")
plt.show()
```

> Como puedes ver, cada n√∫mero est√° escrito de forma distinta.  
> Lo interesante ser√° ver si la m√°quina es capaz de descubrir que hay grupos‚Ä¶ sin saber a√∫n qu√© n√∫mero representa cada imagen.


### üîπ Paso 3 ‚Äî Convertir las im√°genes a vectores y aplicar PCA

Sabemos que cada imagen es una matriz de 8 √ó 8 p√≠xeles.  
Para aplicar algoritmos como PCA y K-Means, necesitamos representar cada imagen como un **vector de 64 componentes** (una fila con todos los p√≠xeles).

> A este proceso se le llama *flattening*, y es simplemente reorganizar los valores de la matriz en una sola fila.

```python
from sklearn.decomposition import PCA

# Aplanar las im√°genes (de matriz 8x8 a vector de 64)
X = digits.data  # matriz de tama√±o (1797, 64)

print("Tama√±o del conjunto de datos:", X.shape)
```

---

Ahora que tenemos los datos como vectores en un espacio de 64 dimensiones, aplicamos **PCA** para reducirlos a solo 2 dimensiones y poder graficarlos.

```python
# Aplicar PCA para reducir de 64D a 2D
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

print("Tama√±o de los datos proyectados:", X_pca.shape)
```

> Cada imagen ahora es un punto en $\mathbb{R}^2$ (un plano), lo que nos permitir√° **ver los datos y buscar grupos visualmente**.


### üîπ Paso 4 ‚Äî Visualizar los d√≠gitos proyectados en 2D

Ya que cada imagen ha sido reducida a un punto en 2 dimensiones, podemos graficar todos los datos y observar si existen **agrupamientos naturales**.

Por ahora, usaremos las **etiquetas verdaderas** solo para colorear los puntos y facilitar la observaci√≥n.  
Recuerda: m√°s adelante usaremos K-Means **sin estas etiquetas**.

```python
# Visualizar los puntos proyectados con colores seg√∫n su etiqueta real
plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=digits.target, cmap="tab10", s=15)
plt.xlabel("Componente principal 1")
plt.ylabel("Componente principal 2")
plt.title("D√≠gitos proyectados a 2D con PCA")
plt.colorbar(scatter, label="Etiqueta real (solo para visualizaci√≥n)")
plt.grid(True)
plt.show()
```

> Como puedes ver, ciertos grupos ya empiezan a distinguirse:  
> algunos d√≠gitos como los "0", "1" o "6" tienden a estar **m√°s juntos**,  
> mientras que otros se mezclan un poco m√°s.

Este gr√°fico nos da una idea visual de que **s√≠ existen estructuras o cl√∫steres** en los datos, aunque a√∫n no hemos usado ning√∫n algoritmo para descubrirlos.

> Y ahora que los podemos ver‚Ä¶ **es momento de dejar que la m√°quina intente agruparlos por s√≠ sola**.


### ü§ñ K-Means: un algoritmo para descubrir grupos

Ya tenemos los datos de nuestras im√°genes de d√≠gitos representados como puntos en un plano.  
Ahora queremos que la m√°quina **descubra por s√≠ sola** los grupos o cl√∫steres de d√≠gitos que est√°n m√°s cerca entre s√≠.

Para esto, usaremos un algoritmo llamado **K-Means**.

---

### üß† ¬øQu√© hace K-Means?

K-Means intenta **dividir los datos en $k$ grupos (cl√∫steres)**, de forma que:

- Los puntos **dentro de un mismo grupo est√©n lo m√°s cerca posible entre s√≠**, y
- Los puntos de **grupos distintos est√©n lo m√°s separados posible**.

Lo hace repitiendo dos pasos simples:

1. **Asignaci√≥n**: cada punto se asigna al **centroide m√°s cercano**.
2. **Actualizaci√≥n**: cada centroide se recalcula como el **promedio de todos los puntos asignados** a su grupo.

Este proceso se repite hasta que los grupos ya no cambian.

---

### üìç ¬øQu√© es un centroide?

El **centroide** es el punto que representa ‚Äúel centro‚Äù de un grupo.  
Matem√°ticamente, si un grupo contiene los puntos $x_1, x_2, ..., x_n$, su centroide es:

```math
\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i
```

> Es el mismo concepto de punto medio o punto de equilibrio que estudiamos en c√°lculo, pero aplicado aqu√≠ al conjunto de puntos en un cl√∫ster.

---

> Lo interesante es que K-Means **no conoce las etiquetas reales** (los n√∫meros) ‚Äî solo ve los vectores en el plano y trata de organizarlos.

Ahora vamos a aplicarlo con $k = 10$, ya que sabemos que nuestro conjunto contiene 10 clases distintas (del 0 al 9),  
y observaremos si la m√°quina logra descubrir esos grupos usando √∫nicamente distancias y promedios.


### üîπ Paso 5 ‚Äî Aplicar K-Means y visualizar los cl√∫steres

Usaremos la implementaci√≥n de `KMeans` de `scikit-learn`, indicando que queremos dividir los datos en **10 grupos**.

```python
from sklearn.cluster import KMeans

# Aplicar K-Means con 10 cl√∫steres
kmeans = KMeans(n_clusters=10, random_state=42)
kmeans.fit(X_pca)

# Etiquetas asignadas por el algoritmo
labels = kmeans.labels_

# Coordenadas de los centroides descubiertos
centroids = kmeans.cluster_centers_
```

---

Ahora graficamos los puntos con colores seg√∫n el cl√∫ster al que fueron asignados,  
y dibujamos los centroides en el plano para ver d√≥nde est√°n ubicados.

```python
plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap="tab10", s=15)
plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, marker='X', label='Centroides')
plt.xlabel("Componente principal 1")
plt.ylabel("Componente principal 2")
plt.title("Cl√∫steres encontrados por K-Means")
plt.legend()
plt.grid(True)
plt.show()
```

> Los puntos de colores representan los grupos descubiertos.  
> Las **X negras** son los **centroides**, calculados como el promedio de cada grupo de puntos.

---

A pesar de no haber visto ninguna etiqueta real, el algoritmo ha agrupado muchos d√≠gitos similares juntos.  
Esto muestra c√≥mo, usando solo la noci√≥n de **distancia** y el concepto de **centroide**, una m√°quina puede **organizar y entender datos complejos**.



### üîç Paso 6 ‚Äî Comparar los cl√∫steres con las etiquetas reales

Aunque K-Means **no us√≥ las etiquetas reales**, podemos comparar los cl√∫steres que encontr√≥ con las verdaderas clases (los d√≠gitos del 0 al 9) para ver **qu√© tan bien agrup√≥** los datos.

Usaremos una **matriz de contingencia** para comparar las asignaciones de K-Means con las etiquetas originales.

```python
from sklearn.metrics import confusion_matrix
import seaborn as sns
import pandas as pd

# Crear matriz de confusi√≥n entre etiquetas reales y cl√∫steres asignados
conf_mat = confusion_matrix(digits.target, labels)

# Mostrar como heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_mat, annot=True, fmt="d", cmap="Blues", xticklabels=range(10), yticklabels=range(10))
plt.xlabel("Cl√∫ster asignado por K-Means")
plt.ylabel("Etiqueta real")
plt.title("Matriz de comparaci√≥n: etiquetas reales vs cl√∫steres")
plt.show()
```

---

### üìä M√©trica de rendimiento m√°s intuitiva: exactitud por mayor√≠a de voto

Para cada cl√∫ster hallado por K-Means, podemos ver qu√© d√≠gito real aparece m√°s frecuentemente dentro de ese cl√∫ster.  
Luego, asumimos que el cl√∫ster representa ese d√≠gito, y calculamos **qu√© porcentaje del total fue correctamente asignado**.

```python
from scipy.stats import mode
import numpy as np

# Crear un arreglo vac√≠o para las predicciones corregidas
new_labels = np.zeros_like(labels)

# Para cada cl√∫ster, asignar la etiqueta real m√°s com√∫n (voto mayoritario)
for i in range(10):
    mask = labels == i
    if np.any(mask):  # evitar cl√∫steres vac√≠os
        new_labels[mask] = mode(digits.target[mask], keepdims=False).mode

# Calcular el accuracy final
accuracy = np.mean(new_labels == digits.target)
print(f"Exactitud (accuracy) del agrupamiento: {accuracy:.2%}")
```

> Esta m√©trica responde directamente a la pregunta:  
> **¬øQu√© porcentaje de las im√°genes fueron correctamente agrupadas, si interpretamos cada cl√∫ster como un n√∫mero?**

---

### üß† Conclusi√≥n

A pesar de no haber usado etiquetas reales, K-Means logr√≥ identificar grupos que **en muchos casos corresponden bien con los d√≠gitos reales**.

- Algunos cl√∫steres coinciden fuertemente con una sola clase (por ejemplo, los "0" o los "1").
- Otros se mezclan m√°s, especialmente si dos d√≠gitos se escriben de forma parecida.

Esto muestra tanto el poder como la limitaci√≥n del aprendizaje no supervisado:

> ‚úÖ K-Means puede descubrir patrones sin saber qu√© representan.  
> ‚ö†Ô∏è Pero no siempre entiende el significado sem√°ntico de los datos, solo su geometr√≠a.

En este laboratorio hemos visto c√≥mo **una idea sencilla como el centroide**, que ya dominamos en c√°lculo, se convierte en una herramienta central para que una m√°quina pueda **entender y organizar datos reales**.


## üß† Preguntas para reflexionar

Antes de cerrar el laboratorio, responde brevemente las siguientes preguntas. No se espera una respuesta perfecta: lo importante es que puedas expresar lo que comprendiste y lo que te llam√≥ la atenci√≥n.

---

1. ¬øPor qu√© representamos las im√°genes como vectores antes de usarlas en el algoritmo?

2. ¬øPara qu√© usamos PCA en este laboratorio? ¬øQu√© problema resuelve?

3. ¬øQu√© papel cumple el **centroide** en el algoritmo K-Means? ¬øEn qu√© se parece al que usamos en c√°lculo?

4. ¬øK-Means us√≥ las etiquetas reales para agrupar los datos? ¬øC√≥mo logr√≥ formar grupos sin saber qu√© n√∫mero era cada imagen?

5. El algoritmo agrup√≥ correctamente cerca del 57‚ÄØ% de los datos. ¬øTe parece un buen resultado? ¬øPor qu√©?

6. ¬øEn qu√© otros contextos del mundo real podr√≠as aplicar un algoritmo como K-Means?

